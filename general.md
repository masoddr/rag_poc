## 1. Build index
# üß† R√®gles RAG ‚Äì Construction d‚ÄôIndex (PDF/TXT)

Lorsque tu construis ou modifies la partie "build index" du projet RAG (ingestion, embeddings, FAISS):

1. **Ingestion**
   - Utilise `PyPDFLoader` pour les PDF et `TextLoader` pour les TXT.
   - Nettoie le texte : retire sauts de page, caract√®res de contr√¥le et espaces multiples.
   - Conserve les m√©tadonn√©es essentielles : `source`, `page`.

2. **Chunking**
   - Utilise `RecursiveCharacterTextSplitter` avec :
     ```python
     chunk_size=800
     chunk_overlap=120
     separators=["\n\n", ".", "!", "?", ";", ":"]
     ```
   - D√©coupe uniquement apr√®s phrase compl√®te ou paragraphe.
   - √âvite les chunks trop longs (>1000) ou trop courts (<200).

3. **Embeddings**
   - Mod√®le par d√©faut : `sentence-transformers/all-MiniLM-L6-v2`.
   - Si corpus technique, proposer `intfloat/e5-large-v2` ou `multi-qa-mpnet-base-dot-v1`.

4. **Indexation**
   - Stocke l‚Äôindex FAISS persistant (ex: `.faiss_index/`).
   - Toujours sauvegarder apr√®s `build` ou `upsert`.
   - Si index existe : charge, puis ajoute (`upsert_documents`) au lieu de recr√©er.
   - V√©rifie qu‚Äôaucun doublon n‚Äôest ins√©r√© (bas√© sur hash du contenu).

5. **Logs**
   - Log le nombre de documents, chunks, et la taille finale de l‚Äôindex.
   - En cas d‚Äôerreur d‚Äôencodage PDF, ignorer le fichier et continuer.

6. **Qualit√©**
   - V√©rifie que `load_faiss_index()` ne casse pas si dossier vide.
   - Le code doit √™tre typ√© (PEP484) et suivre PEP8.
   - Le pipeline doit √™tre reproductible et idempotent.

7. **Test rapide**
   - Apr√®s build, v√©rifier :
     ```python
     db.similarity_search("mot cl√© test", k=3)
     ```
     pour s‚Äôassurer que l‚Äôindex renvoie bien les bons passages.

# Objectif
Construire un index FAISS robuste, propre et r√©utilisable, garantissant un retrieval pr√©cis pour les futures requ√™tes de Q/A.

## 2. Interrogation de l'index
# üó£Ô∏è RAG ‚Äì Interrogation de l‚ÄôIndex (Retrieval + Prompt)

Lorsque tu modifies ou construis la partie "question answering" du pipeline RAG :

1. **Chargement**
   - Charge l‚Äôindex FAISS via `load_faiss_index()`.
   - L√®ve une erreur claire si le dossier `.faiss_index` est manquant.
   - Instancie le retriever :
     ```python
     retriever = db.as_retriever(search_kwargs={"k": int(os.getenv("RETRIEVAL_TOP_K", 4))})
     ```

2. **Recherche**
   - R√©cup√®re les top-k documents les plus pertinents.
   - V√©rifie que `k` reste mod√©r√© (3‚Äì6).
   - Utilise la similarit√© cosinus par d√©faut (celle de FAISS).
   - Si aucun r√©sultat pertinent, renvoie une r√©ponse neutre (‚ÄúJe ne sais pas‚Äù).

3. **Assemblage du contexte**
   - Concat√®ne les passages trouv√©s avec leurs m√©tadonn√©es :
     ```python
     context = "\n\n".join([f"Source: {d.metadata.get('source')} (page {d.metadata.get('page', '?')})\n{d.page_content}" for d in docs])
     ```
   - Tronque le contexte si > 3 000 tokens avant envoi au LLM.

4. **Prompt**
   - Formate toujours le prompt ainsi :
     ```python
     f"""
     Tu es un assistant factuel. R√©ponds uniquement √† partir du contexte ci-dessous.
     Si le contexte ne contient pas la r√©ponse, dis simplement "Je ne sais pas".

     Contexte:
     {context}

     Question:
     {question}

     R√©ponds de fa√ßon claire et concise, en citant les sources utilis√©es.
     """
     ```
   - √âvite toute reformulation ou invention hors contexte.

5. **LLM (Ollama)**
   - Utilise `ChatOllama` ou `Ollama` selon la version LangChain.
   - Param√®tres via `.env` :
     ```bash
     OLLAMA_BASE_URL=http://localhost:11434
     OLLAMA_MODEL=llama3.1:8b
     ```
   - Si la VRAM est limit√©e, recommande `mistral:7b` ou `phi3:mini`.

6. **Post-traitement**
   - Retourne un JSON structur√© :
     ```python
     {
       "answer": str,
       "sources": [{"source": str, "page": int|null}]
     }
     ```
   - Nettoie les r√©ponses : pas de markdown parasite ni de doublons dans les sources.

7. **Observabilit√©**
   - Log la question, le nombre de chunks r√©cup√©r√©s et la dur√©e totale.
   - Mesure la latence d‚Äôappel LLM pour √©valuer les perfs.

# Objectif
Obtenir des r√©ponses pr√©cises, sourc√©es et sans hallucination, en combinant retrieval FAISS + prompting contr√¥l√© vers le LLM Ollama.
